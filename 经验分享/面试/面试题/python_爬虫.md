# Python 爬虫

## 基础

### Python 中常见的爬虫库有哪些？

+ Requests
+ Beautiful Soup
+ Scrapy
+ Selenium

### 什么是 robots.txt 文件？

+ robots.txt 文件是一个网站的根目录下的文本文件，用于指示搜索引擎爬虫哪些页面可以爬取，哪些不可以。它是一种遵循协议的方式，以确保爬虫不会访问不希望被爬取的页面。

## Web 自动化

### 如何处理网页中的动态内容？

+ 处理动态内容的方法包括使用 Selenium 等工具，模拟浏览器行为，或者分析网页中的 AJAX 请求，然后直接获取数据。